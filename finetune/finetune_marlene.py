"""
ë§ˆë¥¼ë Œ ìºë¦­í„° íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ë°ë¹„ì™€ ê°™ì€ ë°©ì‹ì´ì§€ë§Œ ë§ˆë¥¼ë Œ ë°ì´í„°ë¡œ í•™ìŠµ
"""

from unsloth import FastLanguageModel
import torch
from datasets import Dataset
import json
from trl import SFTTrainer
from transformers import TrainingArguments

# 1ë‹¨ê³„: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ”¥ ë§ˆë¥¼ë Œ ëª¨ë¸ ë¡œë”©ì¤‘...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b-instruct-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# 2ë‹¨ê³„: LoRA ì„¤ì •
print("âš™ï¸ LoRA ì„¤ì •ì¤‘...")
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# 3ë‹¨ê³„: ë§ˆë¥¼ë Œ ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ“š ë§ˆë¥¼ë Œ ë°ì´í„°ì…‹ ë¡œë”©ì¤‘...")
with open('training_data/marlene_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

def format_prompt(sample):
    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{sample['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>

{sample['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{sample['output']}<|eot_id|>"""

formatted_data = [{"text": format_prompt(sample)} for sample in data]
dataset = Dataset.from_list(formatted_data)

print(f"âœ… ë§ˆë¥¼ë Œ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! ì´ {len(dataset)}ê°œ ìƒ˜í”Œ")

# 4ë‹¨ê³„: íŠ¸ë ˆì´ë‹
print("ğŸ‹ï¸ ë§ˆë¥¼ë Œ íŠ¸ë ˆì´ë‹ ì‹œì‘...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=30,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs/marlene_model",
        save_steps=10,
        save_total_limit=2,
    ),
)

# 5ë‹¨ê³„: íŒŒì¸íŠœë‹ ì‹¤í–‰
print("ğŸš€ ë§ˆë¥¼ë Œ íŒŒì¸íŠœë‹ ì‹œì‘!")
trainer_stats = trainer.train()

print("ğŸ‰ ë§ˆë¥¼ë Œ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
print(f"ìµœì¢… loss: {trainer_stats.training_loss}")

# 6ë‹¨ê³„: ëª¨ë¸ ì €ì¥
print("ğŸ’¾ ë§ˆë¥¼ë Œ ëª¨ë¸ ì €ì¥ì¤‘...")
model.save_pretrained("marlene_finetuned")
tokenizer.save_pretrained("marlene_finetuned")

print("âœ… ë§ˆë¥¼ë Œ ëª¨ë¸ ì™„ë£Œ! 'marlene_finetuned' í´ë”ì— ì €ì¥ëì–´ìš”!")

# 7ë‹¨ê³„: í…ŒìŠ¤íŠ¸
print("\nğŸ§ª ë§ˆë¥¼ë Œ í…ŒìŠ¤íŠ¸:")
FastLanguageModel.for_inference(model)

test_input = "ì•ˆë…•í•˜ì„¸ìš”!"
inputs = tokenizer([format_prompt({"instruction": "You are Marlene", "input": test_input, "output": ""})], 
                   return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ì…ë ¥: {test_input}")
print(f"ë§ˆë¥¼ë Œ ì‘ë‹µ: {response.split('assistant')[-1].strip()}")