"""
ë°ë¹„ ìºë¦­í„° íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ì´ˆë³´ìë„ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ ì£¼ì„ì„ ë§ì´ ë‹¬ì•˜ì–´ìš”!
"""

from unsloth import FastLanguageModel
import torch
from datasets import Dataset
import json
from trl import SFTTrainer
from transformers import TrainingArguments

# 1ë‹¨ê³„: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ”¥ ëª¨ë¸ ë¡œë”©ì¤‘...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b-instruct-bnb-4bit",  # ì‘ê³  ë¹ ë¥¸ ëª¨ë¸
    max_seq_length=2048,
    dtype=None,  # ìë™ìœ¼ë¡œ ê°ì§€
    load_in_4bit=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
)

# 2ë‹¨ê³„: LoRA ì„¤ì • (íŒŒì¸íŠœë‹ìš©)
print("âš™ï¸ LoRA ì„¤ì •ì¤‘...")
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA ë­í¬ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# 3ë‹¨ê³„: ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë”©ì¤‘...")
with open('training_data/debi_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# 4ë‹¨ê³„: ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
def format_prompt(sample):
    """ëŒ€í™” í˜•íƒœë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{sample['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>

{sample['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{sample['output']}<|eot_id|>"""

# ë°ì´í„°ì…‹ ë³€í™˜
formatted_data = [{"text": format_prompt(sample)} for sample in data]
dataset = Dataset.from_list(formatted_data)

print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! ì´ {len(dataset)}ê°œ ìƒ˜í”Œ")

# 5ë‹¨ê³„: íŠ¸ë ˆì´ë‹ ì„¤ì •
print("ğŸ‹ï¸ íŠ¸ë ˆì´ë‹ ì‹œì‘...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    args=TrainingArguments(
        per_device_train_batch_size=2,  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
        gradient_accumulation_steps=4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
        warmup_steps=5,
        max_steps=30,  # ì§§ê²Œ í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œë¡œëŠ” 100-500 ì¶”ì²œ)
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs/debi_model",  # ëª¨ë¸ ì €ì¥í•  í´ë”
        save_steps=10,
        save_total_limit=2,
    ),
)

# 6ë‹¨ê³„: ì‹¤ì œ íŒŒì¸íŠœë‹ ì‹¤í–‰!
print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘! (ì‹œê°„ì´ ì¢€ ê±¸ë ¤ìš”...)")
trainer_stats = trainer.train()

print("ğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
print(f"ìµœì¢… loss: {trainer_stats.training_loss}")

# 7ë‹¨ê³„: ëª¨ë¸ ì €ì¥
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ì¤‘...")
model.save_pretrained("debi_finetuned")
tokenizer.save_pretrained("debi_finetuned")

print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! 'debi_finetuned' í´ë”ì— ì €ì¥ëì–´ìš”!")

# 8ë‹¨ê³„: í…ŒìŠ¤íŠ¸í•´ë³´ê¸°
print("\nğŸ§ª í…ŒìŠ¤íŠ¸ í•´ë³´ê¸°:")
FastLanguageModel.for_inference(model)

test_input = "ì•ˆë…•í•˜ì„¸ìš”!"
inputs = tokenizer([format_prompt({"instruction": "You are Debi", "input": test_input, "output": ""})], 
                   return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ì…ë ¥: {test_input}")
print(f"ë°ë¹„ ì‘ë‹µ: {response.split('assistant')[-1].strip()}")