{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Eternal Return VLM Fine-tuning\n",
        "\n",
        "이터널리턴 전문가 VLM (데비&마를렌 봇) 파인튜닝 노트북\n",
        "\n",
        "## 목표\n",
        "- 이터널리턴 아이템/캐릭터/특성 이미지 인식\n",
        "- Tool Use 패턴 학습\n",
        "- 데비&마를렌 페르소나 적용\n",
        "\n",
        "## 모델\n",
        "- **SmolVLM2-2.2B** (Colab 무료 T4에서 가능)\n",
        "- 또는 **Qwen2.5-VL-3B** (더 좋은 성능)"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 환경 설정"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# 필수 패키지 설치\n",
        "!pip install -q transformers>=4.45.0\n",
        "!pip install -q trl>=0.12.0\n",
        "!pip install -q peft>=0.13.0\n",
        "!pip install -q accelerate>=0.34.0\n",
        "!pip install -q bitsandbytes>=0.44.0\n",
        "!pip install -q datasets\n",
        "!pip install -q pillow\n",
        "!pip install -q wandb  # 학습 로깅용 (선택)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "gpu_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터셋 업로드\n",
        "\n",
        "로컬에서 생성한 데이터셋과 이미지를 업로드해야 합니다.\n",
        "\n",
        "### 필요한 파일:\n",
        "1. `eternal_return_vlm_dataset.json` - 학습 데이터\n",
        "2. `emojis/` 폴더 - 이미지들 (zip으로 압축해서 업로드)"
      ],
      "metadata": {
        "id": "data_upload"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트 (데이터 업로드용)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 경로 설정\n",
        "# Google Drive에 업로드한 경우:\n",
        "DATASET_PATH = \"/content/drive/MyDrive/eternal_return_vlm/eternal_return_vlm_dataset.json\"\n",
        "IMAGES_DIR = \"/content/drive/MyDrive/eternal_return_vlm/emojis\"\n",
        "\n",
        "# 또는 직접 업로드한 경우:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # dataset.json 업로드\n",
        "# DATASET_PATH = \"eternal_return_vlm_dataset.json\"\n",
        "\n",
        "import os\n",
        "print(f\"Dataset exists: {os.path.exists(DATASET_PATH)}\")\n",
        "print(f\"Images dir exists: {os.path.exists(IMAGES_DIR)}\")"
      ],
      "metadata": {
        "id": "data_path"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 데이터셋 로드 및 전처리"
      ],
      "metadata": {
        "id": "data_load"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# 데이터셋 로드\n",
        "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"총 데이터: {len(raw_data)}개\")\n",
        "print(f\"샘플: {raw_data[0]}\")"
      ],
      "metadata": {
        "id": "load_json"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"이미지 로드 (상대 경로 처리)\"\"\"\n",
        "    if image_path is None:\n",
        "        return None\n",
        "    full_path = os.path.join(IMAGES_DIR, \"..\", image_path)\n",
        "    if os.path.exists(full_path):\n",
        "        return Image.open(full_path).convert(\"RGB\")\n",
        "    return None\n",
        "\n",
        "def format_conversation(item):\n",
        "    \"\"\"대화를 학습 형식으로 변환\"\"\"\n",
        "    conversations = item.get(\"conversations\", [])\n",
        "    \n",
        "    # 이미지가 있는 대화와 없는 대화 구분\n",
        "    image = load_image(item.get(\"image\"))\n",
        "    \n",
        "    # 대화를 텍스트로 변환\n",
        "    text_parts = []\n",
        "    for conv in conversations:\n",
        "        role = conv[\"from\"]\n",
        "        value = conv[\"value\"]\n",
        "        \n",
        "        if role == \"human\":\n",
        "            text_parts.append(f\"User: {value}\")\n",
        "        elif role == \"gpt\":\n",
        "            text_parts.append(f\"Assistant: {value}\")\n",
        "        elif role == \"tool\":\n",
        "            text_parts.append(f\"[Tool Result]: {value}\")\n",
        "    \n",
        "    return {\n",
        "        \"image\": image,\n",
        "        \"text\": \"\\n\".join(text_parts)\n",
        "    }\n",
        "\n",
        "# 데이터 변환\n",
        "processed_data = [format_conversation(item) for item in raw_data]\n",
        "\n",
        "# 이미지가 있는 데이터만 필터링 (VLM 학습용)\n",
        "processed_data = [d for d in processed_data if d[\"image\"] is not None]\n",
        "\n",
        "print(f\"이미지가 있는 데이터: {len(processed_data)}개\")"
      ],
      "metadata": {
        "id": "process_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Dataset으로 변환\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list(processed_data)\n",
        "print(dataset)\n",
        "\n",
        "# Train/Test 분할\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "id": "create_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델 로드 (SmolVLM2 or Qwen2.5-VL)"
      ],
      "metadata": {
        "id": "model_load"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForVision2Seq,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# 모델 선택\n",
        "MODEL_ID = \"HuggingFaceTB/SmolVLM-Instruct\"  # 가벼운 모델\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"  # 더 좋은 성능 (VRAM 더 필요)\n",
        "\n",
        "# 4bit 양자화 설정 (VRAM 절약)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 프로세서 로드\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# 모델 로드\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(f\"모델 로드 완료: {MODEL_ID}\")\n",
        "print(f\"모델 메모리: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LoRA 설정"
      ],
      "metadata": {
        "id": "lora_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # 주요 레이어\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 모델 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 학습 가능한 파라미터 확인\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 학습 설정 및 실행"
      ],
      "metadata": {
        "id": "training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# 학습 설정\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./eternal_return_vlm_lora\",\n",
        "    \n",
        "    # 배치 설정\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    \n",
        "    # 학습 설정\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    \n",
        "    # 최적화\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_8bit\",\n",
        "    \n",
        "    # 로깅\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    # 기타\n",
        "    remove_unused_columns=False,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        ")\n",
        "\n",
        "print(\"학습 설정 완료\")"
      ],
      "metadata": {
        "id": "training_args"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 콜레이터 (이미지 처리)\n",
        "def collate_fn(examples):\n",
        "    texts = [example[\"text\"] for example in examples]\n",
        "    images = [example[\"image\"] for example in examples]\n",
        "    \n",
        "    # 프로세서로 인코딩\n",
        "    batch = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "    \n",
        "    # Labels 설정\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "    \n",
        "    return batch"
      ],
      "metadata": {
        "id": "collate_fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer 생성\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "\n",
        "print(\"Trainer 준비 완료!\")"
      ],
      "metadata": {
        "id": "trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작!\n",
        "print(\"학습 시작...\")\n",
        "trainer.train()\n",
        "print(\"학습 완료!\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 모델 저장"
      ],
      "metadata": {
        "id": "save"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA 어댑터 저장\n",
        "SAVE_PATH = \"/content/drive/MyDrive/eternal_return_vlm/lora_adapter\"\n",
        "\n",
        "trainer.save_model(SAVE_PATH)\n",
        "processor.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(f\"모델 저장 완료: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 테스트"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 이미지로 추론\n",
        "from PIL import Image\n",
        "\n",
        "# 테스트 이미지 로드\n",
        "test_image_path = os.path.join(IMAGES_DIR, \"items_graded/202503.png\")  # 성법의\n",
        "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
        "\n",
        "# 프롬프트\n",
        "test_prompt = \"User: <image>\\n이 아이템 뭐야?\\nAssistant:\"\n",
        "\n",
        "# 추론\n",
        "inputs = processor(text=test_prompt, images=test_image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"=\" * 50)\n",
        "print(response)\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Tool Use 테스트"
      ],
      "metadata": {
        "id": "tool_test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool Use 패턴 테스트\n",
        "test_prompt_tool = \"User: <image>\\n이 아이템 스탯 알려줘\\nAssistant:\"\n",
        "\n",
        "inputs = processor(text=test_prompt_tool, images=test_image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"=\" * 50)\n",
        "print(\"Tool Use 테스트:\")\n",
        "print(response)\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# [TOOL_CALL: get_item_stats(...)] 패턴이 나오면 성공!"
      ],
      "metadata": {
        "id": "tool_inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. 다음 단계\n",
        "\n",
        "1. **저장된 LoRA 어댑터 다운로드**\n",
        "   - Google Drive에서 `lora_adapter/` 폴더 다운로드\n",
        "\n",
        "2. **Discord 봇에 통합**\n",
        "   - 로컬 GPU 또는 서버에서 모델 로드\n",
        "   - Tool Call 감지 시 실제 API 호출\n",
        "\n",
        "3. **성능 개선**\n",
        "   - 더 많은 데이터로 재학습\n",
        "   - Qwen2.5-VL-7B로 업그레이드 (RTX 4090에서)"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}
