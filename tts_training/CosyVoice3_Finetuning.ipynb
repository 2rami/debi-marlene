{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosyVoice3 Fine-tuning (Debi & Marlene)\n",
    "\n",
    "순서대로 실행. 모든 torchcodec/PyTorch 호환성 문제 패치 완료.\n",
    "\n",
    "**모델:** Fun-CosyVoice3-0.5B-2512 (한국어 지원)\n",
    "\n",
    "**패치 목록:**\n",
    "1. file_utils.py - torchaudio.load -> soundfile\n",
    "2. processor.py - torchaudio.load -> soundfile  \n",
    "3. train_utils.py - PyTorch distributed 호환성\n",
    "4. extract_embedding.py - kaldi.fbank 80-bin mel\n",
    "5. extract_speech_token.py - whisper 128-bin mel + feats_length\n",
    "6. cosyvoice2.yaml - vocab_size 자동 수정\n",
    "7. data.list - 절대 경로 사용\n",
    "8. 자동 백업 - 5분마다 Google Drive 저장 (세션 종료 대비)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GPU 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CosyVoice 클론\n",
    "!git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git\n",
    "%cd CosyVoice\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. 의존성 설치\n!pip install -q grpcio==1.62.0 grpcio-tools==1.62.0 --only-binary=:all:\n!pip install -q -r requirements.txt --ignore-installed\n!pip install -q modelscope onnxruntime-gpu openai-whisper\n!pip install -q hyperpyyaml hydra-core lightning wget pyworld\n!pip install -q x_transformers conformer\n\n# 호환 버전 고정 (마지막에)\n!pip install -q numpy==1.26.4 scipy==1.11.4 numba==0.59.1 llvmlite==0.42.0 transformers==4.40.0\n\nprint(\"설치 완료 - 런타임 재시작 필요\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 런타임 재시작 (필수!)\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 재시작 후 - 경로 설정 & 모든 패치 적용\n",
    "%cd /content/CosyVoice\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/content/CosyVoice')\n",
    "sys.path.insert(0, '/content/CosyVoice/third_party/Matcha-TTS')\n",
    "\n",
    "# ===== 1. file_utils.py 패치 (torchcodec -> soundfile) =====\n",
    "file_utils_patch = '''\n",
    "import os, json, torch, torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "def read_lists(list_file):\n",
    "    with open(list_file, \"r\", encoding=\"utf8\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def read_json_lists(list_file):\n",
    "    results = {}\n",
    "    for fn in read_lists(list_file):\n",
    "        with open(fn, \"r\", encoding=\"utf8\") as f:\n",
    "            results.update(json.load(f))\n",
    "    return results\n",
    "\n",
    "def load_wav(wav, target_sr, min_sr=16000):\n",
    "    data, sr = sf.read(wav, dtype=\"float32\")\n",
    "    if data.ndim == 1:\n",
    "        speech = torch.from_numpy(data).unsqueeze(0)\n",
    "    else:\n",
    "        speech = torch.from_numpy(data.T).mean(dim=0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        assert sr >= min_sr\n",
    "        speech = torchaudio.transforms.Resample(sr, target_sr)(speech)\n",
    "    return speech\n",
    "\n",
    "def convert_onnx_to_trt(*args, **kwargs): pass\n",
    "def export_cosyvoice2_vllm(*args, **kwargs): pass\n",
    "'''\n",
    "with open('cosyvoice/utils/file_utils.py', 'w') as f:\n",
    "    f.write(file_utils_patch)\n",
    "\n",
    "# ===== 2. processor.py 패치 (torchaudio.load -> soundfile) =====\n",
    "processor_path = 'cosyvoice/dataset/processor.py'\n",
    "with open(processor_path, 'r') as f:\n",
    "    processor_content = f.read()\n",
    "\n",
    "old_load = \"sample['speech'], sample['sample_rate'] = torchaudio.load(BytesIO(sample['audio_data']))\"\n",
    "new_load = \"\"\"# soundfile로 교체 (torchcodec 우회)\n",
    "        import soundfile as sf\n",
    "        audio_data, sr = sf.read(BytesIO(sample['audio_data']), dtype='float32')\n",
    "        if audio_data.ndim == 1:\n",
    "            sample['speech'] = torch.from_numpy(audio_data).unsqueeze(0)\n",
    "        else:\n",
    "            sample['speech'] = torch.from_numpy(audio_data.T).mean(dim=0, keepdim=True)\n",
    "        sample['sample_rate'] = sr\"\"\"\n",
    "\n",
    "if old_load in processor_content:\n",
    "    processor_content = processor_content.replace(old_load, new_load)\n",
    "    with open(processor_path, 'w') as f:\n",
    "        f.write(processor_content)\n",
    "    print(\"processor.py 패치 완료\")\n",
    "\n",
    "# ===== 3. train_utils.py 패치 (PyTorch 버전 호환성) =====\n",
    "train_utils_path = 'cosyvoice/utils/train_utils.py'\n",
    "with open(train_utils_path, 'r') as f:\n",
    "    train_content = f.read()\n",
    "\n",
    "old_timeout = \"timeout=group_join.options._timeout\"\n",
    "new_timeout = \"timeout=datetime.timedelta(seconds=1800)\"\n",
    "\n",
    "if old_timeout in train_content:\n",
    "    train_content = train_content.replace(old_timeout, new_timeout)\n",
    "    with open(train_utils_path, 'w') as f:\n",
    "        f.write(train_content)\n",
    "    print(\"train_utils.py 패치 완료\")\n",
    "\n",
    "# ===== 4. extract_embedding.py 패치 (kaldi.fbank 80-bin) =====\n",
    "embedding_patch = '''\n",
    "import sys, os, argparse, torch\n",
    "sys.path.insert(0, \"/content/CosyVoice\")\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torchaudio.compliance.kaldi as kaldi\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_wav_sf(path, target_sr=16000):\n",
    "    data, sr = sf.read(path, dtype=\"float32\")\n",
    "    if data.ndim > 1: data = data.mean(axis=1)\n",
    "    audio = torch.from_numpy(data).unsqueeze(0)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(sr, target_sr)(audio)\n",
    "    return audio\n",
    "\n",
    "def single_job(utt, utt2wav, ort_session):\n",
    "    audio = load_wav_sf(utt2wav[utt])\n",
    "    feat = kaldi.fbank(audio, num_mel_bins=80, dither=0, sample_frequency=16000)\n",
    "    feat = feat - feat.mean(dim=0, keepdim=True)\n",
    "    embedding = ort_session.run(None, {ort_session.get_inputs()[0].name: feat.unsqueeze(0).numpy()})[0].flatten().tolist()\n",
    "    return utt, embedding\n",
    "\n",
    "def main(args):\n",
    "    utt2wav, utt2spk = {}, {}\n",
    "    with open(f\"{args.dir}/wav.scp\") as f:\n",
    "        for line in f:\n",
    "            p = line.strip().split(maxsplit=1)\n",
    "            if len(p) == 2: utt2wav[p[0]] = p[1]\n",
    "    with open(f\"{args.dir}/utt2spk\") as f:\n",
    "        for line in f:\n",
    "            p = line.strip().split()\n",
    "            if len(p) == 2: utt2spk[p[0]] = p[1]\n",
    "    \n",
    "    option = ort.SessionOptions()\n",
    "    option.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    option.intra_op_num_threads = 1\n",
    "    ort_session = ort.InferenceSession(args.onnx_path, sess_options=option, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "    \n",
    "    utt2embedding, spk2embedding = {}, {}\n",
    "    with ThreadPoolExecutor(max_workers=args.num_thread) as ex:\n",
    "        futures = {ex.submit(single_job, u, utt2wav, ort_session): u for u in utt2wav}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures)):\n",
    "            try:\n",
    "                utt, emb = fut.result()\n",
    "                utt2embedding[utt] = emb\n",
    "                spk = utt2spk[utt]\n",
    "                if spk not in spk2embedding:\n",
    "                    spk2embedding[spk] = []\n",
    "                spk2embedding[spk].append(emb)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    for k, v in spk2embedding.items():\n",
    "        spk2embedding[k] = torch.tensor(v).mean(dim=0).tolist()\n",
    "    \n",
    "    torch.save(utt2embedding, f\"{args.dir}/utt2embedding.pt\")\n",
    "    torch.save(spk2embedding, f\"{args.dir}/spk2embedding.pt\")\n",
    "    print(f\"Saved {len(utt2embedding)} utt embeddings, {len(spk2embedding)} spk embeddings\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dir\", required=True)\n",
    "    parser.add_argument(\"--onnx_path\", required=True)\n",
    "    parser.add_argument(\"--num_thread\", type=int, default=4)\n",
    "    main(parser.parse_args())\n",
    "'''\n",
    "\n",
    "# ===== 5. extract_speech_token.py 패치 (whisper 128-bin + feats_length) =====\n",
    "token_patch = '''\n",
    "import sys, os, argparse, torch, logging\n",
    "sys.path.insert(0, \"/content/CosyVoice\")\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_wav_sf(path, target_sr=16000):\n",
    "    data, sr = sf.read(path, dtype=\"float32\")\n",
    "    if data.ndim > 1: data = data.mean(axis=1)\n",
    "    audio = torch.from_numpy(data).unsqueeze(0)\n",
    "    if sr != target_sr:\n",
    "        audio = torchaudio.transforms.Resample(sr, target_sr)(audio)\n",
    "    return audio\n",
    "\n",
    "def main(args):\n",
    "    utt2wav = {}\n",
    "    with open(f\"{args.dir}/wav.scp\") as f:\n",
    "        for line in f:\n",
    "            p = line.strip().split(maxsplit=1)\n",
    "            if len(p) == 2: utt2wav[p[0]] = p[1]\n",
    "    \n",
    "    option = ort.SessionOptions()\n",
    "    option.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    option.intra_op_num_threads = 1\n",
    "    ort_session = ort.InferenceSession(args.onnx_path, sess_options=option, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "    \n",
    "    utt2token = {}\n",
    "    for utt in tqdm(utt2wav):\n",
    "        try:\n",
    "            audio = load_wav_sf(utt2wav[utt])\n",
    "            if audio.shape[1] / 16000 > 30:\n",
    "                utt2token[utt] = []\n",
    "                continue\n",
    "            feat = whisper.log_mel_spectrogram(audio, n_mels=128)\n",
    "            tokens = ort_session.run(None, {\n",
    "                ort_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\n",
    "                ort_session.get_inputs()[1].name: np.array([feat.shape[2]], dtype=np.int32)\n",
    "            })[0].flatten().tolist()\n",
    "            utt2token[utt] = tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error {utt}: {e}\")\n",
    "    \n",
    "    torch.save(utt2token, f\"{args.dir}/utt2speech_token.pt\")\n",
    "    print(f\"Saved {len(utt2token)} speech tokens\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dir\", required=True)\n",
    "    parser.add_argument(\"--onnx_path\", required=True)\n",
    "    main(parser.parse_args())\n",
    "'''\n",
    "\n",
    "# 패치 파일 저장\n",
    "os.makedirs('examples/libritts/cosyvoice2/tools', exist_ok=True)\n",
    "with open('examples/libritts/cosyvoice2/tools/extract_embedding.py', 'w') as f:\n",
    "    f.write(embedding_patch)\n",
    "with open('examples/libritts/cosyvoice2/tools/extract_speech_token.py', 'w') as f:\n",
    "    f.write(token_patch)\n",
    "\n",
    "os.makedirs('tools', exist_ok=True)\n",
    "with open('tools/extract_embedding.py', 'w') as f:\n",
    "    f.write(embedding_patch)\n",
    "with open('tools/extract_speech_token.py', 'w') as f:\n",
    "    f.write(token_patch)\n",
    "\n",
    "import numpy as np\n",
    "print(f\"모든 패치 완료! numpy: {np.__version__}\")\n",
    "print(\"패치 목록:\")\n",
    "print(\"  1. file_utils.py - soundfile 사용\")\n",
    "print(\"  2. processor.py - soundfile 사용\") \n",
    "print(\"  3. train_utils.py - PyTorch 호환성\")\n",
    "print(\"  4. extract_embedding.py - kaldi.fbank 80-bin\")\n",
    "print(\"  5. extract_speech_token.py - whisper 128-bin + feats_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모델 다운로드 (2512 = 한국어)\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_DIR = '/content/CosyVoice/pretrained_models/CosyVoice3-0.5B'\n",
    "snapshot_download('FunAudioLLM/Fun-CosyVoice3-0.5B-2512', local_dir=MODEL_DIR)\n",
    "print(f\"다운로드 완료: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Google Drive 마운트 & 데이터 복사\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp /content/drive/MyDrive/debi_tts/filtered_transcripts.json /content/\n",
    "!cp /content/drive/MyDrive/debi_tts/Debi_Marlene_KOR.zip /content/\n",
    "!unzip -q -o /content/Debi_Marlene_KOR.zip -d /content/\n",
    "print(\"데이터 복사 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 모델 로드\n",
    "from cosyvoice.cli.cosyvoice import AutoModel\n",
    "import json\n",
    "import soundfile as sf\n",
    "\n",
    "cosyvoice = AutoModel(model_dir=MODEL_DIR)\n",
    "print(f\"모델 로드 성공! Sample Rate: {cosyvoice.sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 대사 데이터 로드\n",
    "import os\n",
    "\n",
    "with open('/content/filtered_transcripts.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "debi_lines = [d for d in data if d['character'] == 'Debi']\n",
    "marlene_lines = [d for d in data if d['character'] == 'Marlene']\n",
    "print(f\"Debi: {len(debi_lines)}개, Marlene: {len(marlene_lines)}개\")\n",
    "\n",
    "def get_wav_path(character, filename):\n",
    "    for p in [f'/content/{character}/{filename}', f'/content/{character}/{filename.replace(\" \", \"_\")}']:\n",
    "        if os.path.exists(p): return p\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Zero-shot 테스트 (Debi)\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "debi_prompt = None\n",
    "for d in debi_lines:\n",
    "    if len(d['text']) > 10 and '!' not in d['text'] and '?' not in d['text']:\n",
    "        wav_path = get_wav_path('Debi', d['path'].split('\\\\')[-1])\n",
    "        if wav_path:\n",
    "            debi_prompt = {'path': wav_path, 'text': d['text']}\n",
    "            break\n",
    "\n",
    "print(f\"프롬프트: {debi_prompt['text']}\")\n",
    "prompt_text = f\"You are a helpful assistant.<|endofprompt|>{debi_prompt['text']}\"\n",
    "test_text = \"안녕! 나는 데비야! 오늘 뭐 할래?\"\n",
    "\n",
    "for i, out in enumerate(cosyvoice.inference_zero_shot(tts_text=test_text, prompt_text=prompt_text, prompt_wav=debi_prompt['path'], stream=False)):\n",
    "    sf.write(f'/content/test_debi_{i}.wav', out['tts_speech'].squeeze().cpu().numpy(), cosyvoice.sample_rate)\n",
    "\n",
    "print(\"[원본]\")\n",
    "display(Audio(debi_prompt['path']))\n",
    "print(\"[생성]\")\n",
    "display(Audio('/content/test_debi_0.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Zero-shot 테스트 (Marlene)\n",
    "marlene_prompt = None\n",
    "for d in marlene_lines:\n",
    "    if len(d['text']) > 10 and '!' not in d['text'] and '?' not in d['text']:\n",
    "        wav_path = get_wav_path('Marlene', d['path'].split('\\\\')[-1])\n",
    "        if wav_path:\n",
    "            marlene_prompt = {'path': wav_path, 'text': d['text']}\n",
    "            break\n",
    "\n",
    "if marlene_prompt:\n",
    "    print(f\"프롬프트: {marlene_prompt['text']}\")\n",
    "    prompt_text = f\"You are a helpful assistant.<|endofprompt|>{marlene_prompt['text']}\"\n",
    "    test_text = \"...시끄러워. 조용히 해.\"\n",
    "    \n",
    "    for i, out in enumerate(cosyvoice.inference_zero_shot(tts_text=test_text, prompt_text=prompt_text, prompt_wav=marlene_prompt['path'], stream=False)):\n",
    "        sf.write(f'/content/test_marlene_{i}.wav', out['tts_speech'].squeeze().cpu().numpy(), cosyvoice.sample_rate)\n",
    "    \n",
    "    print(\"[원본]\")\n",
    "    display(Audio(marlene_prompt['path']))\n",
    "    print(\"[생성]\")\n",
    "    display(Audio('/content/test_marlene_0.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SFT Fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 훈련 데이터 준비\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('/content/CosyVoice/examples/libritts/cosyvoice2/data/debi_marlene')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wav_scp, text_lines, utt2spk = [], [], []\n",
    "spk2utt = {'Debi': [], 'Marlene': []}\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    filename = item['path'].split('\\\\')[-1]\n",
    "    wav_path = get_wav_path(item['character'], filename)\n",
    "    if not wav_path: continue\n",
    "    \n",
    "    utt_id = f\"{item['character']}_{i:04d}\"\n",
    "    wav_scp.append(f'{utt_id} {wav_path}')\n",
    "    text_lines.append(f\"{utt_id} {item['text']}\")\n",
    "    utt2spk.append(f\"{utt_id} {item['character']}\")\n",
    "    spk2utt[item['character']].append(utt_id)\n",
    "\n",
    "(DATA_DIR / 'wav.scp').write_text('\\n'.join(wav_scp), encoding='utf-8')\n",
    "(DATA_DIR / 'text').write_text('\\n'.join(text_lines), encoding='utf-8')\n",
    "(DATA_DIR / 'utt2spk').write_text('\\n'.join(utt2spk), encoding='utf-8')\n",
    "(DATA_DIR / 'spk2utt').write_text('\\n'.join([f\"{k} {' '.join(v)}\" for k,v in spk2utt.items()]), encoding='utf-8')\n",
    "\n",
    "print(f\"Debi: {len(spk2utt['Debi'])}개, Marlene: {len(spk2utt['Marlene'])}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Speaker Embedding 추출\n",
    "!python examples/libritts/cosyvoice2/tools/extract_embedding.py \\\n",
    "    --dir examples/libritts/cosyvoice2/data/debi_marlene \\\n",
    "    --onnx_path {MODEL_DIR}/campplus.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Speech Token 추출\n",
    "!python examples/libritts/cosyvoice2/tools/extract_speech_token.py \\\n",
    "    --dir examples/libritts/cosyvoice2/data/debi_marlene \\\n",
    "    --onnx_path {MODEL_DIR}/speech_tokenizer_v3.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Parquet 변환\n",
    "!mkdir -p examples/libritts/cosyvoice2/data/debi_marlene/parquet\n",
    "!python examples/libritts/cosyvoice2/tools/make_parquet_list.py \\\n",
    "    --num_utts_per_parquet 100 \\\n",
    "    --num_processes 4 \\\n",
    "    --src_dir examples/libritts/cosyvoice2/data/debi_marlene \\\n",
    "    --des_dir examples/libritts/cosyvoice2/data/debi_marlene/parquet\n",
    "\n",
    "!ls examples/libritts/cosyvoice2/data/debi_marlene/parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. 훈련 데이터 리스트 (절대 경로 사용!)\n",
    "%cd /content/CosyVoice/examples/libritts/cosyvoice2\n",
    "\n",
    "import glob\n",
    "\n",
    "# 존재하는 parquet 파일만 절대 경로로\n",
    "parquet_files = sorted(glob.glob('/content/CosyVoice/examples/libritts/cosyvoice2/data/debi_marlene/parquet/*.tar'))\n",
    "data_list = '\\n'.join(parquet_files)\n",
    "\n",
    "with open('data/train.data.list', 'w') as f:\n",
    "    f.write(data_list)\n",
    "with open('data/dev.data.list', 'w') as f:\n",
    "    f.write(data_list)\n",
    "\n",
    "print(f\"Parquet 파일 {len(parquet_files)}개:\")\n",
    "!cat data/train.data.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 17. vocab_size 및 학습 설정 수정\nimport torch\nimport re\n\nMODEL_DIR = '/content/CosyVoice/pretrained_models/CosyVoice3-0.5B'\nckpt = torch.load(f'{MODEL_DIR}/llm.pt', map_location='cpu', weights_only=False)\ntarget_vocab = ckpt['llm_decoder.weight'].shape[0]\nprint(f\"체크포인트 vocab_size: {target_vocab}\")\n\nconfig_path = '/content/CosyVoice/examples/libritts/cosyvoice2/conf/cosyvoice2.yaml'\nwith open(config_path, 'r') as f:\n    content = f.read()\n\n# 1. vocab_size 수정\nmatch = re.search(r'speech_token_size:\\s*(\\d+)', content)\nif match:\n    current = int(match.group(1))\n    offset = 3\n    new_size = target_vocab - offset\n    if current != new_size:\n        content = re.sub(r'speech_token_size:\\s*\\d+', f'speech_token_size: {new_size}', content)\n        print(f\"speech_token_size 수정: {current} -> {new_size}\")\n    else:\n        print(f\"speech_token_size 이미 올바름: {current}\")\n\n# 2. max_epoch 수정 (200 -> 60, 오버피팅 방지)\ncontent = re.sub(r'max_epoch:\\s*\\d+', 'max_epoch: 60', content)\nprint(\"max_epoch: 60으로 설정 (15, 30, 45, 60 체크포인트 저장)\")\n\n# 3. learning rate 수정 (1e-5 -> 5e-6, 더 안정적인 학습)\ncontent = re.sub(r'lr:\\s*[\\d.e-]+', 'lr: 5e-6', content)\nprint(\"lr: 5e-6으로 설정 (더 안정적인 학습)\")\n\n# 4. warmup_steps 수정 (2500 -> 500, 데이터가 적으므로)\ncontent = re.sub(r'warmup_steps:\\s*\\d+', 'warmup_steps: 500', content)\nprint(\"warmup_steps: 500으로 설정\")\n\nwith open(config_path, 'w') as f:\n    f.write(content)\nprint(\"\\n설정 파일 저장 완료!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 18. 자동 백업 시작 (5분마다 Google Drive 저장 - 세션 종료 대비)\nimport os\n\nbackup_script = '''\nimport time, shutil, os\nsrc = \"/content/CosyVoice/examples/libritts/cosyvoice2/exp/debi_marlene\"\ndst = \"/content/drive/MyDrive/debi_tts/cosyvoice3_finetuned/debi_marlene\"\nwhile True:\n    time.sleep(300)  # 5분\n    if os.path.exists(src):\n        try:\n            shutil.copytree(src, dst, dirs_exist_ok=True)\n            print(f\"백업 완료: {dst}\")\n        except Exception as e:\n            print(f\"백업 실패: {e}\")\n'''\nwith open('/content/backup.py', 'w') as f:\n    f.write(backup_script)\n!nohup python /content/backup.py > /content/backup.log 2>&1 &\n\n# 체크포인트 자동 삭제 (15 에포크 단위로 유지: 15, 30, 45, 60) - Flow 폴더\ncleanup_script = '''\nimport time, glob, os, re\n\nd = \"/content/CosyVoice/examples/libritts/cosyvoice2/exp/debi_marlene/flow\"\nkeep_epochs = {15, 30, 45, 60}  # 유지할 에포크\n\nwhile True:\n    time.sleep(60)  # 1분마다 확인\n    files = glob.glob(f\"{d}/epoch_*_whole.pt\")\n    if not files:\n        continue\n    \n    for f in files:\n        # epoch 번호 추출\n        match = re.search(r'epoch_(\\d+)_whole\\.pt', f)\n        if match:\n            epoch = int(match.group(1))\n            # 유지할 에포크가 아니고, 최신 2개에도 포함되지 않으면 삭제\n            sorted_files = sorted(files)\n            if epoch not in keep_epochs and f not in sorted_files[-2:]:\n                try:\n                    os.remove(f)\n                    print(f\"삭제: epoch_{epoch}\")\n                except:\n                    pass\n'''\nwith open('/content/cleanup.py', 'w') as f:\n    f.write(cleanup_script)\n!nohup python /content/cleanup.py > /dev/null 2>&1 &\n\nprint(\"자동 백업 시작 (5분마다 Drive 저장)\")\nprint(\"자동 삭제 시작 (15, 30, 45, 60 에포크 + 최신 2개 유지)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 19. Flow 훈련 (음성 품질/화자 특성 파인튜닝)\n%cd /content/CosyVoice/examples/libritts/cosyvoice2\n\nMODEL_DIR = '/content/CosyVoice/pretrained_models/CosyVoice3-0.5B'\n\n!PYTHONPATH=/content/CosyVoice:/content/CosyVoice/third_party/Matcha-TTS \\\n    torchrun --nproc_per_node=1 --master_port=29501 \\\n    /content/CosyVoice/cosyvoice/bin/train.py \\\n    --train_engine torch_ddp \\\n    --config conf/cosyvoice2.yaml \\\n    --train_data data/train.data.list \\\n    --cv_data data/dev.data.list \\\n    --model flow \\\n    --checkpoint {MODEL_DIR}/flow.pt \\\n    --model_dir exp/debi_marlene/flow \\\n    --tensorboard_dir tensorboard/debi_marlene/flow \\\n    --num_workers 2 \\\n    --prefetch 50 \\\n    --pin_memory \\\n    --use_amp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. 모델 저장 (Google Drive) - 절대 경로 사용\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "src = '/content/CosyVoice/examples/libritts/cosyvoice2/exp/debi_marlene'\n",
    "dst = '/content/drive/MyDrive/debi_tts/cosyvoice3_finetuned/debi_marlene'\n",
    "\n",
    "if os.path.exists(src):\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    print(f\"저장 완료: {dst}\")\n",
    "    !ls -la {dst}/llm/\n",
    "else:\n",
    "    print(f\"소스 폴더 없음: {src}\")\n",
    "    print(\"자동 백업 확인:\")\n",
    "    !ls -la /content/drive/MyDrive/debi_tts/cosyvoice3_finetuned/ 2>/dev/null || echo \"백업 없음\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 21. 파인튜닝 모델 테스트 (Flow)\n%cd /content/CosyVoice\nimport glob, shutil\nimport soundfile as sf\nfrom IPython.display import Audio, display\nfrom cosyvoice.cli.cosyvoice import AutoModel\n\nMODEL_DIR = '/content/CosyVoice/pretrained_models/CosyVoice3-0.5B'\n\n# Flow 체크포인트 찾기 (로컬 또는 Drive 백업)\nckpts = sorted(glob.glob('/content/CosyVoice/examples/libritts/cosyvoice2/exp/debi_marlene/flow/epoch_*_whole.pt'))\nif not ckpts:\n    ckpts = sorted(glob.glob('/content/drive/MyDrive/debi_tts/cosyvoice3_finetuned/debi_marlene/flow/epoch_*_whole.pt'))\n\nif ckpts:\n    print(f\"Flow 체크포인트: {ckpts[-1]}\")\n    shutil.copy(ckpts[-1], f'{MODEL_DIR}/flow.pt')\n    cosyvoice_ft = AutoModel(model_dir=MODEL_DIR)\n    print(f\"Speaker: {cosyvoice_ft.list_available_spks()}\")\n    \n    for i, out in enumerate(cosyvoice_ft.inference_sft(tts_text='안녕! 파인튜닝 완료!', spk_id='Debi', stream=False)):\n        sf.write(f'/content/test_sft_{i}.wav', out['tts_speech'].squeeze().cpu().numpy(), cosyvoice_ft.sample_rate)\n    display(Audio('/content/test_sft_0.wav'))\nelse:\n    print(\"Flow 체크포인트 없음\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}