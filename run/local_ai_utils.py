"""
ë¡œì»¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” AI ìœ í‹¸ë¦¬í‹°
Claude API ëŒ€ì‹  ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
"""

import discord
from typing import Dict, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from run.config import characters

# ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
debi_model = None
debi_tokenizer = None
marlene_model = None  
marlene_tokenizer = None

async def initialize_local_models():
    """ë¡œì»¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
    global debi_model, debi_tokenizer, marlene_model, marlene_tokenizer
    
    try:
        print("ğŸ”¥ ë°ë¹„ ëª¨ë¸ ë¡œë”©ì¤‘...")
        debi_tokenizer = AutoTokenizer.from_pretrained("./debi_finetuned")
        debi_model = AutoModelForCausalLM.from_pretrained(
            "./debi_finetuned",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ë°ë¹„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        print("ğŸ”¥ ë§ˆë¥¼ë Œ ëª¨ë¸ ë¡œë”©ì¤‘...")
        marlene_tokenizer = AutoTokenizer.from_pretrained("./marlene_finetuned") 
        marlene_model = AutoModelForCausalLM.from_pretrained(
            "./marlene_finetuned",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ë§ˆë¥¼ë Œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë¨¼ì € íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")

def format_prompt(character_name: str, user_message: str, context: str = "") -> str:
    """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (íŒŒì¸íŠœë‹í•  ë•Œì™€ ê°™ì€ í˜•íƒœ)"""
    if character_name.lower() == "debi":
        instruction = "You are Debi from Eternal Return. Respond as the cheerful, energetic older twin sister."
    else:  # marlene
        instruction = "You are Marlene from Eternal Return. Respond as the cool, tsundere younger twin sister."
    
    full_input = user_message
    if context:
        full_input = f"ê²Œì„ ì •ë³´: {context}\n\nì‚¬ìš©ì: {user_message}"
    
    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>

{full_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

async def generate_local_ai_response(character: Dict[str, Any], user_message: str, context: str = "") -> str:
    """ë¡œì»¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ AI ì‘ë‹µ ìƒì„±"""
    character_name = character['name']
    
    # ìºë¦­í„°ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
    if character_name == "ë°ë¹„":
        if not debi_model or not debi_tokenizer:
            return "ë°ë¹„ ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ì–´ìš”. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"
        
        model = debi_model
        tokenizer = debi_tokenizer
        model_character = "debi"
        
    elif character_name == "ë§ˆë¥¼ë Œ":
        if not marlene_model or not marlene_tokenizer:
            return "ë§ˆë¥¼ë Œ ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ì–´ìš”. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"
        
        model = marlene_model
        tokenizer = marlene_tokenizer  
        model_character = "marlene"
        
    else:
        return "ì•Œ ìˆ˜ ì—†ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤."
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = format_prompt(model_character, user_message, context)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if torch.cuda.is_available():
            inputs = inputs.to("cuda")
        
        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,  # ì‘ë‹µ ê¸¸ì´
                temperature=0.7,     # ì°½ì˜ì„± ì¡°ì ˆ (0~1)
                do_sample=True,      # ëœë¤ì„± ì¶”ê°€
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1  # ë°˜ë³µ ë°©ì§€
            )
        
        # ì‘ë‹µ ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # assistant ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "<|start_header_id|>assistant<|end_header_id|>" in full_response:
            response = full_response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        else:
            response = full_response[len(prompt):].strip()
        
        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
        if not response:
            if character_name == "ë°ë¹„":
                return "ì™€! ë­”ê°€ ë§ì´ ì•ˆ ë‚˜ì™€! ë‹¤ì‹œ ë§í•´ì¤˜!"
            else:  # ë§ˆë¥¼ë Œ
                return "...ë­ë¼ê³  ë§í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ë„¤."
        
        return response[:500]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        
    except Exception as e:
        print(f"ë¡œì»¬ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        if character_name == "ë°ë¹„":
            return "ì–´? ë­”ê°€ ì´ìƒí•´! ë‹¤ì‹œ ë§í•´ì¤˜!"
        else:  # ë§ˆë¥¼ë Œ
            return "...ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ë„¤."

def create_character_embed(character: Dict[str, Any], title: str, description: str, user_message: str = None, author_image_override: str = None) -> discord.Embed:
    """ìºë¦­í„° ì„ë² ë“œ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
    embed = discord.Embed(
        title=title,
        description=description,
        color=character["color"]
    )
    
    embed.set_author(
        name=character["name"],
        icon_url=author_image_override or character["image"]
    )
    
    if user_message:
        embed.set_footer(text=f"ë©”ì‹œì§€: {user_message}")
    
    return embed