{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 데비&마를렌 VLM 파인튜닝 (Qwen2.5-VL + A100)\n",
        "\n",
        "이터널리턴 전문가 봇 - 데비처럼 답하는 VLM\n",
        "\n",
        "**모델**: Qwen2.5-VL-3B-Instruct\n",
        "**GPU**: A100 필수!\n",
        "\n",
        "예상 시간: 1~2시간"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 패키지 설치\n",
        "!pip install transformers>=4.45.0\n",
        "!pip install trl>=0.12.0\n",
        "!pip install peft>=0.13.0\n",
        "!pip install accelerate>=0.34.0\n",
        "!pip install bitsandbytes>=0.44.0\n",
        "!pip install datasets pillow\n",
        "!pip install qwen-vl-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. GPU 확인\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.0f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Google Drive 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. 경로 설정 & 파일 확인\n",
        "import os\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/eternal_return_vlm\"\n",
        "DATASET_PATH = f\"{DRIVE_PATH}/eternal_return_vlm_dataset.json\"\n",
        "EMOJIS_ZIP = f\"{DRIVE_PATH}/emojis.zip\"\n",
        "\n",
        "print(f\"Dataset: {os.path.exists(DATASET_PATH)}\")\n",
        "print(f\"Emojis: {os.path.exists(EMOJIS_ZIP)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. 이미지 압축 해제\n",
        "!unzip -q -o \"{EMOJIS_ZIP}\" -d /content/\n",
        "!ls /content/emojis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. 데이터셋 로드\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"총 데이터: {len(raw_data)}개\")\n",
        "print(f\"샘플: {raw_data[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. 모델 & 프로세서 로드\n",
        "import torch\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# 4bit 양자화\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(f\"모델: {MODEL_ID}\")\n",
        "print(f\"메모리: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. LoRA 설정\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. 데이터 전처리 (Qwen2.5-VL 형식)\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "DEBI_SYSTEM = \"\"\"너는 데비야! 이터널리턴에서 온 캐릭터고, 지금은 디스코드 봇으로 활동 중이야.\n",
        "성격: 밝고 활발하고 친근해! 마를렌 동생이랑 같이 다녀.\n",
        "역할: 이터널리턴 게임 정보 (아이템, 캐릭터, 특성 등) 알려주기\n",
        "말투: 반말로 친근하게! ~야, ~어, ~지, ~해 같은 어미 사용\"\"\"\n",
        "\n",
        "def load_image(image_path):\n",
        "    if image_path is None:\n",
        "        return None\n",
        "    full_path = f\"/content/{image_path}\"\n",
        "    if os.path.exists(full_path):\n",
        "        return Image.open(full_path).convert(\"RGB\")\n",
        "    return None\n",
        "\n",
        "def convert_to_qwen_format(item):\n",
        "    \"\"\"데이터를 Qwen2.5-VL 메시지 형식으로 변환\"\"\"\n",
        "    image = load_image(item.get(\"image\"))\n",
        "    if image is None:\n",
        "        return None\n",
        "    \n",
        "    conversations = item.get(\"conversations\", [])\n",
        "    \n",
        "    messages = [{\"role\": \"system\", \"content\": DEBI_SYSTEM}]\n",
        "    \n",
        "    for conv in conversations:\n",
        "        role = conv[\"from\"]\n",
        "        value = conv[\"value\"]\n",
        "        \n",
        "        if role == \"human\":\n",
        "            # <image> 태그 제거하고 이미지 따로 처리\n",
        "            text = value.replace(\"<image>\\n\", \"\").replace(\"<image>\", \"\").strip()\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": text}\n",
        "                ]\n",
        "            })\n",
        "        elif role == \"gpt\":\n",
        "            messages.append({\"role\": \"assistant\", \"content\": value})\n",
        "        elif role == \"tool\":\n",
        "            messages.append({\"role\": \"assistant\", \"content\": f\"[Tool Result]: {value}\"})\n",
        "    \n",
        "    return {\"messages\": messages, \"image\": image}\n",
        "\n",
        "print(\"데이터 변환 중...\")\n",
        "processed_data = [convert_to_qwen_format(item) for item in raw_data]\n",
        "processed_data = [d for d in processed_data if d is not None]\n",
        "print(f\"변환 완료: {len(processed_data)}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. Dataset 생성\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list(processed_data)\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "print(f\"Train: {len(dataset['train'])}\")\n",
        "print(f\"Test: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11. Collate Function (Qwen2.5-VL 전용)\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    all_images = []\n",
        "    \n",
        "    for example in examples:\n",
        "        messages = example[\"messages\"]\n",
        "        \n",
        "        # 템플릿 적용\n",
        "        text = processor.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "        \n",
        "        # 이미지 수집\n",
        "        image_inputs, _ = process_vision_info(messages)\n",
        "        if image_inputs:\n",
        "            all_images.extend(image_inputs)\n",
        "    \n",
        "    # 프로세서로 인코딩\n",
        "    batch = processor(\n",
        "        text=texts,\n",
        "        images=all_images if all_images else None,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "    )\n",
        "    \n",
        "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "    return batch\n",
        "\n",
        "print(\"Collate function 준비 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 12. 학습 설정\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./debi_qwen_vlm_lora\",\n",
        "\n",
        "    # A100용 배치 설정\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "\n",
        "    # 학습 설정\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # 최적화\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    # 로깅 & 저장\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # 기타\n",
        "    remove_unused_columns=False,\n",
        "    max_length=1024,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"학습 설정 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13. Trainer 생성\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "\n",
        "print(\"Trainer 준비 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. 학습 시작!\n",
        "print(\"=\"*50)\n",
        "print(\"학습 시작! 예상 시간: 1~2시간\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"학습 완료!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15. 모델 저장\n",
        "SAVE_PATH = f\"{DRIVE_PATH}/debi_qwen_vlm_lora\"\n",
        "\n",
        "trainer.save_model(SAVE_PATH)\n",
        "processor.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(f\"저장 완료: {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 16. 테스트 함수\n",
        "def ask_debi(image_path, question):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": DEBI_SYSTEM},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "    \n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    \n",
        "    # 입력 부분 제외하고 출력만\n",
        "    output_ids = outputs[0][len(inputs.input_ids[0]):]\n",
        "    response = processor.decode(output_ids, skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 17. 테스트!\n",
        "print(\"=\" * 50)\n",
        "print(\"데비 테스트\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 아이템 테스트\n",
        "ask_debi(\"/content/emojis/items_graded/202503.png\", \"이 아이템 뭐야?\")\n",
        "\n",
        "# 캐릭터 테스트\n",
        "ask_debi(\"/content/emojis/characters/Aya.png\", \"이 캐릭터 누구야?\")\n",
        "\n",
        "# Tool Use 테스트\n",
        "ask_debi(\"/content/emojis/items_graded/116409.png\", \"이 아이템 스탯 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 완료!\n",
        "\n",
        "저장 위치: `Drive/eternal_return_vlm/debi_qwen_vlm_lora/`\n",
        "\n",
        "## Discord 봇에서 사용\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "\n",
        "# 베이스 모델\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "\n",
        "# LoRA 어댑터 적용\n",
        "model = PeftModel.from_pretrained(model, \"./debi_qwen_vlm_lora\")\n",
        "```"
      ]
    }
  ]
}
